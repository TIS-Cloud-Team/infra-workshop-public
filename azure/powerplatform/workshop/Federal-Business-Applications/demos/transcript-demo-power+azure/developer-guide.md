# Developer Guide

This document is for developers to help them understand *how* the solution works. Note: not every single control will be detailed.  (e.g. controls like back buttons will not be explained.)
        
## Contents
- [Tables](#tables)
  - [Transcript](#transcript)
  - [Recognized Phrases](#recognized-phrases)
  - [Speaker](#speaker)
- [Demo Transcript app (canvas)](#Demo-Transcript-app-canvas)
  - [App Properties & Settings](#App-Properties-Settings)
  - [Screens](#Screens)
  - [Main Screen](#main-screen)
    - [Properties](#properties)
    - [Controls](#main-screen-controls)
  - [Transcript Demo Screen](#Transcript-Demo-Screen)
    - [Controls](#controls-1)
- [Flows](#flows)
  - [01 - Power Apps - Upload to Azure Blob](#01---power-apps---upload-to-azure-blob)
  - [02 -Child Flow - Create Transcript](#02--child-flow---create-transcript)
    - [02b Child Flow - Loop Until Transcript Complete](#02b-child-flow---loop-until-transcript-complete)
    - [02c Child Flow - Get Transcript Results](#02c-child-flow---get-transcript-results)
    - [02d Child Flow - Parse Transcript and Load into Dataverse](#02d-child-flow---parse-transcript-and-load-into-dataverse)
    - [02e - Child Flow - Summarize Tanscript](#02e---child-flow---summarize-tanscript)
  - [PA - Create Transcript Document](#pa---create-transcript-document)

[← Back to Read Me](readme.md#contents)
***
## Tables
There are three related tables in the solution:  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/482ef37b-b347-474b-860d-0ca7df4d0510)  


### Transcript
A separate record is created for each transcript generated by Azure Batch Speech to Text services.  (i.e. one record for each audio file)

#### Columns
Here is a breakdown of each custom column inclulding data type and description
| **Display Name**      | **Schema Name**           | **Data Type**                 | **Description** |
| :-----------        | :----------            | :--------                  | :---------- |
| Duration            | demo_duration          | Decimal                    | Flow converts ticks to seconds | 
| Duration in Ticks   | demo_durationinticks   | Single line of text        | Use text data type because ticks are too large to store in whole number (integer) and Big Integer type has issues when importing solution |
| Source File Format  | demo_sourcefileformat  | Single line of text (fx)   | Power FX column that displays the last three characters of the file name ( e.g. mp3 or wav) |
| Source File Name    | demo_sourcefilename    | Single line of text        | Name of the audio file uploaded |
| Source File Size    | demo_sourcefilesize    | Whole Number               | File size of the audio file |
| Source URL          | demo_sourcurl          | URL                        | SAS URL of the audio file (generated via flow) |
| Time Stamp          | demo_timestamp         | Date and time              | Time and date from the transcription services |
| Transcript          | demo_transcriptid      | Unique identifier          | Unique GUID of the this record |
| Transcript Number   | demo_transcriptnumber  | # Autonumber               | Auto generated number used as the Primary Name for the record |


### Recognized Phrases
A separate record is created for each recognized phrase determined by Azure Batch Speech-To-Text services.  
#### Columns
Here is a breakdown of each custom column including data type and description
| **Display Name**      | **Schema Name**           | **Data Type**                 | **Description** |
| :-----------        | :----------            | :--------                  | :---------- |
| Display  | demo display  | Multiple lines of text  | Actual text of the current phrase determined by Azure Batch Speech to Text servies |
| Duration in Seconds  | demo_durationinseconds  | Decimal  | Flow converts  Duration in Ticks to seconds |
| Duration in Ticks  | demo_durationinticks   | Single line of text        | Use text data type because ticks are too large to store in whole number (integer) and Big Integer type has issues when importing solution | 
| Offset in Seconds  | demo_offsetinseconds  | Decimal  | Flow converts Offset in Ticks to seconds |
| Offset in Ticks    | demo_offsetinticks |  Single of text  | Use text data type because ticks are too large to store in whole number (integer) and Big Integer type has issues when importing solution | 
| Outset  | demo_outset  | Decimal (FX) | Power FX formula column that determines the outset (or out point) for the current phrase in seconds. Forumula: ```'Duration in Seconds'+'Offset in Seconds'``` |
| Phrase Number  | demo_phrasenumber  | # Autonumber  | Auto generated number used as the Primary Name for the record |
| Recognized Phrases  | demo_recognizedphrasesid  | Unique identifier          | Unique GUID of the this record |
| Speaker  | demo_speaker |  Whole number  | When diarization is enabled for Azure Batch Speech to Text, it returns a number for each unique speaker it detects.  | 
| Speaker Lookup  | demo_speakerlookup  | Lookup |  Related table: Speaker.  This relates the phrase to a speaker record (created by the user) |
| Transcript  | demo_transcript  | Lookup | Related table: Transcript.  This relates the phrase to it's parent Transcript record |


### Speaker
User can create a record for each speaker involved in a transcript.  
#### Columns
Here is a breakdown of each custom column including data type and description
| **Display Name**      | **Schema Name**           | **Data Type**                 | **Description** |
| :-----------        | :----------            | :--------                  | :---------- |
| Name  | demo_name  | Single line of text  | Speaker name and Primary column |
| Speaker  | demo_speakerid   | Unique identifier | Unique GUID of the this record |

## Demo Transcript app (canvas)
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/60e1f755-59e2-4748-b463-d7b5233b9846)

### App Properties & Settings
Several settings and properties were changed.  Note that any preview features should **not** be used for production apps. 

**Display -> Scale To Fit**  
Set to off to allow for responsive resizing. Recommend if different form factors maybe used by users (e.g. phone, tablet, laptop)  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/0478df24-4d16-491d-955d-f353d21ae58d)

**General -> Modern controls and themes**  
Set to on to allow for modern controls/themes in the app.  Note that some modern controls are in GA and others are still in preview at this time.   
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/608b1497-841b-4e9b-bbd6-79c6b5c062ce)

[▲Top](#contents)

### Screens
The canvas app has two screens: 
1. [Main Screen](#main-screen)
2. [Transcript Demo Screen](#Transcript-Demo-Screen)

Both screens use containers to help control the flow of the controls when resizing the app for different resolutions.  Their layouts are based on the Sidebar screen template.

![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/1ba43b2a-0c9e-4413-a5f2-f9153b7716f4)

[▲Top](#contents)

### Main Screen
This screen is used to upload audio files and select transcripts to view/edit. 
![Screenshot of Power Apps Studio with Main Screen selected](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/64881327-c8dd-40be-b663-14290ac78cae)

#### Properties

- **OnVisible**: When the screen loads, several global variables are set:
  - **glbShowSpinner**: Used to show/hide loading spinner
  - **glbSpinnerLabel**: Used to display next next to loading spinner
  - **glbShowSuccess**: Used to show/hide the Success message when upload is completed
  - **glbSelectedFileName**: Stores the file name of the selected file
  - **glbSelectedTranscript**: Stores the selected transcript from from the left hand gallery (galTranscripts_Main)
  - **glbCurrentPhrase**: Used on the next screen, to identify the current recogonized phrase based on the current playback point in the audio controller
  - **glbMode**: Used on the next screen to toggle between Edit and View display modes via the **Edit** Button
- **StartScreen** : ```'Main Screen'```
- **Formulas** : Sets static variables
  ```
  // Supported file formats:
  colSupportedFileFormats = ["mp3","wav","aac","opus","ogg","flac","wma","amr","webm","m4a","speex"];
  //Source: https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-transcription-audio-data?tabs=portal#supported-audio-formats-and-codecs
  
  // List of supported formats:
  glbListSupportedFileFormats = Concat(colSupportedFileFormats,ThisRecord.Value & ", ");
  ```

In addition to setting variables, two controls are reset (see below for more on each control):
- **attFileToUploadMain**
- **inpTotalSpeakersMain**
  <a name="main-screen-controls">
#### Controls

- **cont_Main_1_Vert** </br> Only visible when **glbShowSpinner = true** 
- **cont_Main_1_1_Horiz** </br>Creates rounded rectangle with drop shadow                                                
- **spinner_Main** </br> OOTB Spinner (modern) control
- **cont_Main_2_Vert** </br> Parent container for all other controls below 
- **cont_Main_2_1_Horiz** </br>Contains the app header                                   
- **headerMain** </br>OOTB Header (modern) control                                               
- **cont_Main_2_2_Horiz** </br> Parent container that contains every control below the header
- **shpSpacerLeftMain** </br>Only visible when there are no Transcripts found.  Helps center the cont_Main_2_2_1_Vert container.
- **cont_Main_2_2_1_Vert** </br>Contains the controls needed to upload a new audio file
- **attFileToUpload** </br> This control allows user to upload a file. The control validates file size and file format:  
  - The API has a limit of 100 Mb, so the control is limited to 100 Mb. 
  - The Speech to Text API only accepts the following formats:
    - MP3 
    - WAV 
    - AAC 
    - OPUS 
    - OGG 
    - FLAC 
    - WMA 
    - AMR 
    - WEBM 
    - M4A 
    - SPEEX                                                                           
  - It has several properties customized:
    - **AccessibleLabel**: ```"File to attach (upload) and transcribe"```
    - **AddAttachmentText**: ```"Select audio file (100 MB Max)"```
    - **Color**: This changes color to red if selected file is not supported audio file
    ```
    If(
    //IF Attachment is supported format
    Lower(
        Right(
            First(attFileToUploadMain.Attachments).Name,
            3
        )
    ) in colSupportedFileFormats,
    // THEN Color is Black
    Color.Black,
    // IF no attachment is selected
    IsEmpty(Self.Attachments),
    // THEN color is black
    Color.Black,
    // ELSE (Error) color is red
    Color.Red
    )
    ```
    - **Height**: ```100```
    - **MaxAttachments**: ```1```
    - If you want to allow for batch uploads, increase this option, but performance may suffer for larger files. Also some other parts of the solution may need to be refactored if you allow more than 1 file at a time
    - **MaxAttachmentSize**: ```1000```
    - _In MB_
    - **MaxAttachmentText**: This code does some basic data validation to check if the selected file is supported format
    ```
    If(
    //IF Attachment is supported format
        Lower(
            Right(
                First(attFileToUploadMain.Attachments).Name,
                3
            )
        ) in colSupportedFileFormats,
    "File Selected. Please click Upload",
    // ELSE display error
    "Error:  Only these file formats are supported: " &  Replace(glbListSupportedFileFormats,Len(glbListSupportedFileFormats)-1,1,"")
    )
    ```
    - **NoAttachmentsText**: ```"There is nothing selected."```
    - **OnAddFile**: Stores the selected file in a global variable (glbSelectedFileName).
    _IF you allow for more than one attachment, you'll need to refactor this_
    ```
    Set(
    glbSelectedFileName,
    First(attFileToUploadMain.Attachments).Name
    )
    ```
    - **OnRemoveFile**: When file is removed, clears variable
    ```
    Set(
    glbSelectedFileName,
    Blank()
    )
    ```
    - **Width**: ```Parent.Width - 60 ```                                                                                        
                        
- **inpTotalSpeakersMain** </br>Number input field that indicates how many speakers should Azure Speech to Text services look for.
  - **AccessibleLabel**: ```"Enter the total number of speakers in the audio file"```
  - **Max**: ```36```
  - Azure Speech To Text services has a limit of 36 speakers for diarization
  - **Min**: ```1```
  - **Value**: ```0``` 

- **cont_Main_2_2_1_1_Horiz** </br>ontains the buttons to upload audio file (or cancel action) 
  - **btnUploadFile_Main** </br> Used to upload the selected file to Azure Blob Storage (via Power Automate flow)
    - **AccessibleLabel**: ```"Upload the selected file"```
    - **DisplayMode**: Default mode disabled. Only enabled (Edit mode) when file is attached, the format is correct and the total speakers is greater than zero        
    ```
        // If total speakers isn't set (min 1), disable this button
        If(
            inpTotalSpeakersMain.Value>0,
            // If No attachment, disable button
            If(
                IsEmpty(attFileToUploadMain.Attachments),
                DisplayMode.Disabled,
                //IF Attachment is supported format
                Lower(
                    Right(
                        First(attFileToUploadMain.Attachments).Name,
                        3
                    )
                ) in colSupportedFileFormats,
                // THEN Enable button
                DisplayMode.Edit,
                //ELSE disable button
                DisplayMode.Disabled
            ),
            //ELSE disable button
            DisplayMode.Disabled
        )
    ```   

    - **OnSelect**: When clicked, shows the loading spinner and calls the Power Automate Flow 
        ```
        // Show the loading spinner
        Set(
            glbShowSpinner,
            true
        );
        //Set loading spinner label
        Set(
            glbSpinnerLabel,
            "Uploading..."
        );
        // Store response from flow in variable (glbResponseUpload)
        Set(
            glbResponseUpload,
            //Run flow to upload the file and kickoff the transcript process
            '01-PowerApps-UploadtoAzureBlob'.Run(
                inpTotalSpeakersMain.Value,
                {
                    file: {
                        name: First(attFileToUploadMain.Attachments).Name,
                        contentBytes: First(attFileToUploadMain.Attachments).Value
                    }
                }
            )
        );
        // Check for error uploading file
        IfError(
            glbResponseUpload,
            //Notify user of error
            Notify(
                "Error: " & FirstError.Message,
                NotificationType.Error
            );
            //Hide the loading spinner
        Set(
                glbShowSpinner,
                false
            ),
        //IF Successful then
            Concurrent(
            //Hide the loading spinner
                Set(
                    glbShowSpinner,
                    false
                ),
            //Show the success message
                Set(
                    glbShowSuccess,
                    true
                ),
                //Reset Attachment Control
                Reset(attFileToUploadMain),
                // Reset Total Speakers input
                Reset(inpTotalSpeakersMain)
            )
        )
        ```
    - **Text**: ```"Upload"```
        
    - **btnCancelUpload_Main** </br>Resets the controls (attFileToUploadMain, inpTotalSpeakersMain)
    - **OnSelect**:
        ```
        //Reset upload attachment and total number of speakers controls
        Reset(attFileToUploadMain);
        // Reset the total speakers input field
        Reset(inpTotalSpeakersMain)
        ```
    - **Text**: ```"Cancel"```
                                                                                
- **cont_Main_2_2_2_Vert** </br> Contains controls to display all transcripts available

- **galTranscripts_Main** </br> Displays **all** the available transcripts in the Transcripts table.
  
   Some properties were customized:
  - **AccessibleLabel**: ```"List of all the transcripts"```
  - **Items**: ```SortByColumns(Transcripts,"createdon",SortOrder.Descending)```
  - **LayoutMinHeight**: ```284```
  - **TemplateSize**: ```274```
  - **Width**: ```Parent.Width-Parent.PaddingLeft-Parent.PaddingRight-Parent.LayoutGap```

- **cont_Main_2_2_2_1_Horiz** </br> Contains all controls for individual transcript records

- **cont_Main_2_2_2_1_1_Vert** </br>  Contains controls to display the transcript details</br>  
  Including:
  - lblTranscriptFileName_Main
  - lblTranscriptDetails_Main
  - lblTranscriptSummary
 
- **btnEditTranscript_Main** </br> Selects the transcript and opens it in the [Transcript Demo Screen](#transcript-demo-screen)
  - **AccessibleLabel**: ```"Click view and edit this transcript"```
  - **Appearance**: ```'ButtonCanvas.Appearance'.Secondary```
  - **Icon**: ```"MoreHorizontal"```
  - **Layout**: ```'ButtonCanvas.Layout'.IconAfter```
  - **OnSelect**: When item is selected, store a copy of the **Recognized Phrases** for the selected transcript in a collection (**colPhrases**) and sort the collection in acensinding order by the '**Offset in Seconds**', then store the selected Transcript record in a global varilable (**glbSelectedTranscript**), then set a variable (**glbCurrentPhrase**) to the first phrase of colPhrases,  then go to the **Transcript Demo Screen**  
     ```
     //Set spinner label and show spinner
    Set(
        glbSpinnerLabel,
        "Loading..."
    );
    Set(
        glbShowSpinner,
        true
    );
    //When item is selected
    //Store a copy of the Recognized Phrases for the selected transcript in a collection (colPhrases)
    // And sort the collection in acensinding order by the 'Offset in Seconds'
    ClearCollect(
        colPhrases,
        SortByColumns(
            Filter(
                ShowColumns(
                    'Recognized Phrases',
                    Display,
                    'Duration in Seconds',
                    'Duration in Ticks',
                    'Offset (HH:MM:SS)',
                    'Offset in Seconds',
                    'Offset in Ticks',
                    Outset,
                    'Outset (HH:MM:SS)',
                    'Phrase Number',
                    Speaker,
                    'Speaker Lookup',
                    Transcript,
                    'Recognized Phrases'
                ),
                demo_Transcript.Transcript = ThisItem.Transcript
            ),
            "demo_offsetinseconds",
            SortOrder.Ascending
        )
    );
    //Store the currently selected transcript in a global variable
    Set(
        glbSelectedTranscript,
        ThisItem
    );
    //Set current phrase (glbCurrentPhrase) to the first item in the phrases collection (colPhrases)
    Set(
        glbCurrentPhrase,
        First(colPhrases)
    );
    //Navigate to the Transcript Demo Screen
    Navigate('Transcript Demo Screen');
    //Hide Spinner
    Set(
        glbShowSpinner,
        false
    );
  
     ```
  - **Text**: ```Details```
  - **Width**: ```100```

- **btnRefreshTranscript_Main** </br> Refreshes the **Transcripts** table
  - **AccessibleLabel**: ```"Refresh the list of transcripts"```
  - **AlignInContainer**: ```AlignInContainer.Center```
  - **Appearance**: ```'ButtonCanvas.Appearance'.Secondary```
  - **Icon**: ```"ArrowClockwise"```
  - **Text**: ```"Refresh"```
  - **OnSelect**: ```Refresh(Transcripts)```

- **cont_Main_2_2_3_Vert** </br>  Only visible after an upload is completed successfully (i.e. **glbShowSuccess = true**)
- **htmlSuccessMain** </br>  HTML formatted success message
- **btnBackSuccessMain** </br>Takes user "back" to Main screen. i.e. sets **glbShowSuccess** to **false**
- **shpSpacerRightMain** </br>Only visible when there are no Transcripts found.  Helps center the cont_Main_2_2_1_Vert container. 



[▲Top](#contents)

---------


### Transcript Demo Screen
This screen has several containers. Some of these are used to for pop-up windows, while most are used to structure the controls.  

![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/1da2865b-c81e-433a-851c-5fba650bbbfc)

#### Controls

All controls (except two) are stored in horizontal and vertical containers to allow for responsive design when the user's screen resolution and aspect ratio change.  

- **pdfFileTranscript**</br>Displays the PDF version of the transcript. Only visible when Transcript File is attached to record and the Trancript (PDF) tab is selected
  - **Document**: ```LookUp(Transcripts,Transcript=glbSelectedTranscript.Transcript).'Transcript File'.Value```
  - **Height**: ```shpFileTranscript.Height```
  - **Visible**: Only displays when selected transcript has PDF attached and "Transcript (PDF)" tab is selected
      ```
    !IsBlank(
        LookUp(
            Transcripts,
            Transcript = glbSelectedTranscript.Transcript
        ).'Transcript File'.Value
    ) And tabMainTranscript.Selected.Value = "Transcript (PDF)"
      ```
  - **Width**: ```contFileTranscript.Width-contFileTranscript.RadiusBottomRight*2```
  - **x**: ```contMainBodyTranscriptHoriz.X+contMainBodyTranscriptHoriz.RadiusBottomLeft```
  - **y**: ```contMainBodyTranscriptHoriz.Y+contMainBodyTranscriptHoriz.RadiusBottomLeft```
  - **Zoom**: ```Zoom.FitHeight```

- **timerTranscript** </br> Used to update variables based on the playhead of the audio control (**audRecordingPlayback**).  
  - **Duration**: This is in milliseconds. 1000 = 1 second  
    ```1000```
  - **OnTimerStart**: Every second, update the current phrase (glbCurrentPhrase)
    ```
    Set(
        glbCurrentPhrase,
        LookUp(
            ShowColumns(
                colPhrases,
                Display,
                'Offset in Seconds',
                Outset,
                'Phrase Number',
                Speaker,
                'Speaker Lookup',
                Transcript,
                'Recognized Phrases',
                'Duration in Seconds',
                'Offset (HH:MM:SS)',
                'Outset (HH:MM:SS)'
            ),
            // Current phrase is between the offset in seconds and the outset
            demo_offsetinseconds <= Trunc(audRecordingPlayback.Time) And demo_outset >= Trunc(audRecordingPlayback.Time)
        )
    )
     ```
  - **Repeat**: ```true```
  - **Start**: ```glbStartTimer```
  - **Visible**: ```false```

- **txtSummaryTranscript** </br> Used to display and edit AI generated summary of transcript
    - **AccessibleLabel**: ```"AI Generated summary using Azure OpenAI"```
    - **Appearance**: ```If(glbMode=DisplayMode.Edit,'TextInputCanvas.Appearance'.FilledDarker,'TextInputCanvas.Appearance'.FilledLighter)```
    - **DisplayMode**: ```glbMode```
    - **Mode**: ```'TextInputCanvas.Mode'.Multiline```
    - **Value**: ```LookUp(Transcripts,Transcript=glbSelectedTranscript.Transcript).Summary```
    - **Width**: ```Parent.Width-Parent.PaddingLeft-Parent.PaddingRight```
  
![audRecordingPlayback](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/4d4feed5-64ae-4b99-bb2c-9d0fe8815037)

- **audRecordingPlayback** </br> Used to playback the original audio (stored in Azure Blob Storage)
  - **AccessibleLabel**: ```"Playback control for the selected transcript's original audio recording"```
  - **DisplayMode**: If user is editing the current phrase, disable this so they can't move the playhead (and change the current phrase)
    ```
    If(
      glbMode = DisplayMode.Edit,
      DisplayMode.Disabled,
      DisplayMode.Edit
     )
     ```
  - **Fill**: ```PowerAppsTheme.Colors.Primary```
    - Note: PowerAppsTheme is the default theme.  You can replace the default theme with your own.  
       ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/05fc98ea-a851-426d-b878-1ca3d53fea08)
  - **Media**: ```glbSelectedTranscript.'Source URL'```
   - **OnEnd**: ```Set(glbStartTimer,false);```
   - **OnPause**: ```Set(glbStartTimer,false);```
   - **OnStart**: ```Set(glbStartTimer,true);```
   - **StartTime**: ```glbJumpToTime```
   - **Width**: ```Parent.Width```

![btnEdit_Transcript](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/21d4b135-ab8f-4761-8b3f-9527367f5cd6)
- **btnEdit_Transcript** </br> Used to put screen into edit mode
_Note: only visible when **NOT** in edit mode and glbSelectedTranscript is NOT blank_
  - **AccessibleLabel**: ```"Edit the current phrase"```
  - **Appearance**: ```'ButtonCanvas.Appearance'.Transparent```
  - **DisplayMode**: ```If (tabMainTranscript.Selected.Value="Transcript (PDF)",DisplayMode.Disabled,DisplayMode.Edit)```
  - **Icon**: ```"Edit"```
  - **IconStyle**: ```'ButtonCanvas.IconStyle'.Outline```
  - **OnSelect**:
    ```
    Set(
      glbMode,
      DisplayMode.Edit
    )
    ```
  - **Text**: ```"Edit"```
  - **Visible** ```Not(glbMode=DisplayMode.Edit) And !IsBlank(glbSelectedTranscript)```

![btnSave_Transcript](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/6b7670e6-c99f-4693-9158-2262df8cd618)
- **btnSave_Transcript** </br>Used to save changes</br> _Note: only visible when in edit mode_  
  - **AccessibleLabel**: ```"Save edits to current phrase"```
  - **Appearance**: ```'ButtonCanvas.Appearance'.Transparent```
  - **Icon**: ```"Save"```
  - **IconStyle**: ```'ButtonCanvas.IconStyle'.Outline```
  - **OnSelect**: Button behaves differently depending on the tab selected. See comments for more details 
    ```
     If(
        //If Playback tab is selected
        tabMainTranscript.Selected.Value = "Playback",
        //Display popup and determine if every instance of the current Speaker value (e.g. 1) should be set to the selected speaker value for all records in this transcript
        If(
            !IsBlank(drpSelectSpeaker_Transcript.Selected),
            Set(
                gblShowPopUpUpdateAllSpeakers,
                true
            ),
        //Otherwise just save changes
            Select(btnSaveHidden)
        ),
        // If Summary tab is selected
        tabMainTranscript.Selected.Value = "Summary",
        //Set spinner label
        Set(glbSpinnerLabel,"Saving");
        // Show spinnner
        Set(
            glbShowSpinner,
            true
        );
        //Update current Transcript record with updated summary and update variable (glbSelectedTranscript) with result
    Set(
            glbSelectedTranscript,
            Patch(
                Transcripts,
                LookUp(
                    Transcripts,
                    Transcript = glbSelectedTranscript.Transcript
                ),
                {Summary: txtSummaryTranscript.Value}
            )
        );
        // If No Errors...
    If(
            IsEmpty(Errors(Transcripts)),
        //.... Reset display mode to View
            Set(
                glbMode,
                DisplayMode.View
            )
        );
        //Hide Spinner
    Set(
            glbShowSpinner,
            false
        );
        
    )
    ```
  - **Text**: ```"Save"```
  - **Visible**: ```glbMode=DisplayMode.Edit```

![btnCancel_Transcript](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/c26cdcec-d1a9-494e-b872-d759a875108d)

- **btnCancel_Transcript** </br>Resets screen to display mode without saving changes</br> _Note: Only visible when in Edit mode_  
  - **AccessibleLabel**: ```"Cancel the edits to the current phrase"```
  - **Appearance**: ```'ButtonCanvas.Appearance'.Outline```
  - **Icon**: ```"Dismiss"```
  - **IconStyle**: ```"Outline"```
  - **OnSelect**: Resets controls and app to View mode
    ```
    Set(
      glbMode,
      DisplayMode.View
    );
    Reset(drpSelectSpeaker_Transcript);
    Reset(txtCurrentPhrase_Transcript)
    ```
  - **Text**: ```"Cancel"```
  - **Visible**: ```glbMode=DisplayMode.Edit```


![txtCurrentPhrase_Transcript](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/98c3429a-cd78-4f37-b203-525eab121cf1)

- **txtCurrentPhrase_Transcript** </br>Displays the text of the currently selected phrase
  - **AccessibleLabel**: ```"Transcript of the current phrase (based current time code)"```
  - **DisplayMode**: If variable glbMode is blank, default to View mode
    ```Coalesce(glbMode,DisplayMode.View)```
  - **FontSize**: ```20```
  - **Mode**: ```'TextInputCanvas.Mode'.Multiline```
  - **Value**: Return the current phrase (glbCurrentPhrase) Display column value
    ```
    glbCurrentPhrase.Display
    ```
  - **Width**: ```Parent.Width```

![lblCurrentSpeaker_Transcript](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/22d1f4da-33fc-46bc-9526-fd5e9653ae52)

- **lblCurrentSpeaker_Transcript** </br>Displays either Speaker name (if available) or speaker number (as generated by Azure Speech to Text)
  - **Text**: If the speaker dropdown has a selected name, use that. If not, use the value of the speaker name from the current phrase (glbCurrentPhrase). If no name exists, get the speaker value (number) from the current phrase
    ```
    "Speaker: " & Coalesce(
      drpSelectSpeaker_Transcript.Selected.Name,
      glbCurrentPhrase.demo_SpeakerLookup.Name,
      glbCurrentPhrase.demo_speaker
    )
    '''
  - **Visible**: ```!IsBlank(glbCurrentPhrase)```

![drpSelectSpeaker_Transcript](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/8e22e6c4-70e6-4328-8a15-2e41d61326d8)

- **drpSelectSpeaker_Transcript** </br>
  - **AccessibleLabel**: ```"Select speaker from this drop down"```
  - *Items**: ```Filter(Speakers, Transcript.Transcript=glbSelectedTranscript.Transcript)```
    - _Only returns speakers already related to current transcript._
  - **Visible**: ```glbMode=DisplayMode.Edit```
    - _Only visible when app is in Edit mode_*    


![icoClearSelectSpeaker_Transcript](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/c58b5e2f-3201-4899-8fb8-263c48b76709)

- **icoClearSelectSpeaker_Transcript** </br>Used to select speaker from **Speakers** table</br> _Note: only visible when speaker is selected in dropdown_  
  - **AccessibleLabel**: ```"Clear selected speaker dropdown"```
  - **BorderStyle**: ```BorderStyle.None```
  - **Color**: ```PowerAppsTheme.Colors.Primary```
  - **Fill**: ```ColorValue("#F5F5F5")```
  - **Icon**: ```Icon.Cancel```
  - **Height**: ```drpSelectSpeaker_Transcript.Height```
  - **OnSelect**: ```Reset(drpSelectSpeaker_Transcript)```
  - **PaddingTop**: ```10```
  - **PaddingBottom**, **PaddingRight**, **PaddingLeft**: ```Self.PaddingTop```
  - **Visible**: ```!IsBlank(drpSelectSpeaker_Transcript.Selected)```
  - **Width**: ```Self.Height```


![btnNewSpeaker_Transcript](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/08151b97-e2bb-404b-9638-b85eed23d579)

- **btnNewSpeaker_Transcript** </br> Opens the **contPopUpUpdateAllSpeakersBg** pop-up
  - **AccessibleLabel**: ```"Add new speaker"```
  - **Appearance**: ```'ButtonCanvas.Appearance'.Subtle```
  - **OnSelect**: Show the Add Speaker Pop Up
    ```
    Set(
        glbShowPopUpAddSpeaker,
        true
    )
    ```
  - **Text**: ```"+ New Speaker"```
  - **Visible**: ```glbMode=DisplayMode.Edit```


![btnJumpToInPoint](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/28f01fac-1537-425b-99f8-f7c3c532327f)

- **btnJumpToInPoint** </br> Sets variable (glbJumpToTime) to current phrase's offset in seconds
  - **AccessibleLabel**: ```"Jump to In Point"```
  - **Appearance**: ```'ButtonCanvas.Appearance'.Outline```
  - **OnSelect**: ```Set(glbJumpToTime,glbCurrentPhrase.demo_offsetinseconds)```
  - **Text**: ```"↦"1```
  - **Visible**: ```glbMode=DisplayMode.View```


![lblInPoint_Transcript](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/483a374f-8295-48de-b13d-a6f060a828ed) 

- **lblInPoint_Transcript** </br> Display the current phrase's in point (Offset in Seconds) in HH:MM:SS format 
  - **Text**: 
     ```
     " In: " & Text(
      RoundDown(
          glbCurrentPhrase.demo_offsetinseconds / 3600,
          0
      ),
      "00"
      ) & ":" & Text(
          RoundDown(
              If(
                  Mod(
                      glbCurrentPhrase.demo_offsetinseconds,
                      3600
                  ) > 0,
                  Mod(
                      glbCurrentPhrase.demo_offsetinseconds,
                      3600
                  ) / 60,
                  glbCurrentPhrase.demo_offsetinseconds / 60
              ),
              0
          ),
          "00"
      ) & ":" & Text(
          Mod(
              glbCurrentPhrase.demo_offsetinseconds,
              60
          ),
          "00"
        )
      ```
  - **Visible**: ```!IsBlank(glbCurrentPhrase)```
  - **Width**: ```120```


![lblOutpoint_Transcript](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/d11bac0b-e3fc-4b3b-8dbf-72ce79db931a) 

- **lblOutpoint_Transcript** </br> Display current phrase's out point (outset) in HH:MM:SS
  - **Align**: ```'TextCanvas.Align'.End```
  - **Text**: 
    ```
     "Out: " & Text(
        RoundDown(
            glbCurrentPhrase.demo_outset / 3600,
            0
        ),
        "00"
    ) & ":" & Text(
        RoundDown(
            If(
                Mod(
                    glbCurrentPhrase.demo_outset,
                    3600
                ) > 0,
                Mod(
                    glbCurrentPhrase.demo_outset,
                    3600
                ) / 60,
                glbCurrentPhrase.demo_outset / 60
            ),
            0
        ),
        "00"
    ) & ":" & Text(
        Mod(
            glbCurrentPhrase.demo_outset,
            60
        ),
        "00"
    ) & " "
    ```
  - **Visible**: ```!IsBlank(glbCurrentPhrase)```
  - **Width**: ```120```

![lblJumpToTime_Transcript](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/1a360425-3a9f-48af-af57-909b9ed26a7e) 

- **lblJumpToTime_Transcript** </br>
  - **Align**: ```'TextCanvas.Align'.End```
  - **FontColor**: If variable glbJumpToTime exceeds the total duration of the audio file, display red text
    ```
    If(
      glbJumpToTime > RoundUp(
          glbSelectedTranscript.Duration,
          0
      ),
      Color.Red,
      Color.Black
    )
    ```
  - **Text**: If variable glbJumpToTime exceeds the total duration of the audio file, display error message, otherwise "Jump To"
    ```
    If(
      glbJumpToTime > RoundUp(
          glbSelectedTranscript.Duration,
          0
      ),
      "Cannot exceed total duration ",
      "Jump To "
    )
    ```
![txtJumpToTime_Transcript](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/d847eef6-59ae-4fa9-bcd6-d2b8efd336d1)  

- **txtJumpToTime_Transcript** </br>Allows user to type time code (HH:MM:SS) to jump to part of recording (and transcript)
  - **AccessibleLabel**: ```"Type the time you want to jump to (Hours:Minutes:Seconds)"```
  - **FontColor**:
    ```
    If(
      glbJumpToTime > RoundUp(
          glbSelectedTranscript.Duration,
          0
      ),
      Color.Red,
      Color.Black
    )
    ```
  - **OnChange**: 
    ```
    //If string value doesn't equal 8 (the length of the string 00:00:00), do nothing
    If(
        Len(Self.Value) = 8,
        //Convert string (HH:MM:SS format) into seconds and store in variable glbJumpToTime
        Set(
            glbJumpToTime,
            Value(
                Left(
                    Self.Value,
                    2
                )
            ) * 3600 + Value(
                Mid(
                    Self.Value,
                    4,
                    2
                )
            ) * 60 + Value(
                Right(
                    Self.Value,
                    2
                )
            )
        );
        //Refresh current phrase
        Set(
            glbCurrentPhrase,
            LookUp(
                colPhrases,
            // Current phrase is between the offset in seconds and the outset
                demo_offsetinseconds <= Trunc(audRecordingPlayback.Time) And demo_outset >= Trunc(audRecordingPlayback.Time)
            )
        )
    )
    ```
  - **Value**: Display current playback time in HH:MM:SS
    ```
    Text(
        RoundDown(
            audRecordingPlayback.Time / 3600,
            0
        ),
        "00"
    ) & ":" & Text(
        RoundDown(
            audRecordingPlayback.Time / 60,0
        ),
        "00"
    ) & ":" & Text(
        Mod(
            audRecordingPlayback.Time,
            60
        ),
        "00"
    )
    ```
  - **Width**: ```100```


![cont_Transcript_3_Vert](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/edbb1d88-5c0b-46b0-9255-ef5f0dceb74e)

- **cont_Transcript_3_Vert** </br>Only visible when user clicks + New Speaker button (btnNewSpeaker_Transcript)
  - **Fill**: ```RGBA(255, 255, 255, 1)```
  - **Height**: ```txtCurrentPhrase_Transcript.Height-10```
  - **PaddingLeft**: ```10```
  - **PaddingRight**: ```5```
  - **PaddingTop**: ```5```
  - **Border Radius** (**RadiusBottomLeft**, **RadiusBottomRight**, **RadiusTopLeft**, **RadiusTopRight**): ```20```
  - **Visible**: ```glbShowPopUpAddSpeaker```
  - **X**: ```contMainBodyTranscriptHoriz.Width+contMainBodyTranscriptHoriz.X-Self.Width```
  - **Y**: ```contMainBodyTranscriptHoriz.Y+contSpeakerTranscriptHoriz.Height+5```
  
- **frmAddSpeaker** </br>Submits new speaker name to Speakers table  
  - **DataSource**: ```Speakers```
  - **DefaultMode**: ```FormMode.New```
  - **OnSelect**: Reset form and set glbShowPopUpAddSpeaker to false
    ```
    Set(
      glbShowPopUpAddSpeaker,
      false
    );
    ResetForm(frmAddSpeaker)
    ```

    
![btnAddSpeakerSave](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/221e4be6-985b-468d-8d2d-be65ae77c297)

- **btnAddSpeakerSave** </br> Submits the add speaker form (frmAddSpeaker)
  - **AccessibleLabel**: ```"Save the new speaker"```
  - **DisplayMode**: If Name field is blank, disable button
    ```
    If(
      IsBlank(Name_DataCard_Value.Value),
      DisplayMode.Disabled,
      DisplayMode.Edit
    )
    ```
  - **OnSelect**: ```SubmitForm(frmAddSpeaker);```
  - **Text**: ```"Save"```


![btnAddSpeakerCancel](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/f288d3da-a2be-4203-928c-625bb8efc987)

- **btnAddSpeakerCancel** </br> Closes and resets the add speaker form (fromAddSpeaker)
  - **AccessibleLabel**: ```"Cancel adding the new speaker"```
  - **Appearance**: ```'ButtonCanvas.Appearance'.Secondary```
  - **OnSelect**: Reset form and hide the Add New Speaker pop-up
    ```
    Set(
      glbShowPopUpAddSpeaker,
      false
    );
    ResetForm(frmAddSpeaker)
    ```
  - **Text**: ```"Cancel"```

- **cont_Transcript_2_Vert** </br>Full screen container that has an opqaue fill and is only visible when **gblShowPopUpUpdateAllSpeakers** = **true**
  - **Fill**: ```RGBA(255, 255, 255, 0.65)```
  - **Height**: Parent.Height
  - **LayoutAlignItems**: ```LayoutAlignItems.Center```
  - **LayoutJustifyContent**: ```LayoutJustifyContent.Center```
  - **Visible**: ```gblShowPopUpUpdateAllSpeakers```
  - **Width**: ```Parent.Width```

![cont_Transcript_2_1_Vert](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/341528f9-2ae8-46e5-92d5-63041fda8e0a)

- **cont_Transcript_2_1_Vert**
  - **DropShadow**: ```DropShadow.ExtraBold```
  - **Fill**: ```RGBA(255, 255, 255, 1)```
  - **Height**: ```Self.Width*.6```
  - **LayoutAlignItems**: ```LayoutAlignItems.Center```
  - **LayoutJustifyContent**: ```LayoutJustifyContent.Center```
  - **Border Radius** (**RadiusBottomLeft**, **RadiusBottomRight**, **RadiusTopLeft**, **RadiusTopRight**): ```25```


![btnPopUpUpdateAllSpeakersYes](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/5c28730e-97a4-47e8-840c-fec5aa5db699)

- **btnPopUpUpdateAllSpeakersYes** </br>
  - **AccessibleLabel**: Dynamcially update accessible label based on the current phrase and the selected speaker name
    ```
    "Yes - For all the speakers equal to " &
    glbCurrentPhrase.demo_speaker &
    " please update Speaker (Lookup) to  " &
    drpSelectSpeaker_Transcript.Selected.Name
    ```
  - **OnSelect**:
    ```
    //If user clicks Yes button
    //THEN hide the Update All Speakers pop-up
    Set(
        gblShowPopUpUpdateAllSpeakers,
        false
    );
    //THEN set flag (glbUpdateAllSpeakers) to true
    Set(
        gblUpdateAllSpeakers,
        true
    );
    //Then select the hidden button (with the actual save formulas)
    Select(btnSaveHidden);
    ```
  - **Text**: ```"Yes"```


![btnPopUpUpdateAllSpeakersNo](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/d0d07751-9058-4a01-aa02-44ad298e468f)  

- **btnPopUpUpdateAllSpeakersNo** </br>
  - **AccessibleLabel**: ```"Please do not update all speakers to the selected speaker"```
  - **Appearance**: ```'ButtonCanvas.Appearance'.Secondary```
  - **OnSelect**:
    ```
    //IF user clicks No
    //THEN hide the Update All Speakers pop-up
    Set(
        gblShowPopUpUpdateAllSpeakers,
        false
    );
    //THEN set flag (gblUpdateAllSpeakers) to FALSE
    Set(
        gblUpdateAllSpeakers,
        false
    );
    //Then select the hidden button (with the actual save formulas)
    Select(btnSaveHidden);
    ```
  - **Text**: ```"No"```

- **btnSaveHidden** </br>This button is hidden, but is called by various other buttons.  This is one technique to create reusable code/functions in Power Apps  
  - **AccessibleLabel**: ```"This is a hidden control - used for saving the edits to the current phrase"```
  - **OnSelect**: 
    ```
    /*
        This section of code is used to update the current recognized phrase.  
        Optionally: if the Speaker was updated, it also can loop through and update all instances of a Speaker Lookup for the selected Speaker. 
        This loop only happens if the user clicks Yes when the Update All Speakers popup appears
    */
    // Show Spinner
    Set(
        glbShowSpinner,
        true
    );
    //Patch the current recognized phrase with contents of the Current Phrase input (txtCurrentPhrase)
    Patch(
        'Recognized Phrases',
        LookUp(
            'Recognized Phrases',
            'Offset in Seconds' <= Int(audRecordingPlayback.Time) And Outset >= Round(
                audRecordingPlayback.Time,
                2
            )
        ),
        {
            Display: txtCurrentPhrase_Transcript.Value,
            'Speaker Lookup': drpSelectSpeaker_Transcript.Selected
        }
    );
    // If user selected "Yes" to Update All Speakers, then loop through and update every phrase record with selected speaker
    If(
        gblUpdateAllSpeakers,
        // Collect all Recognized Phrases that match the current speaker (e.g. 1)
        ClearCollect(
            colUpdateAllPhrasesForSelectedSpeaker,
            Filter(
                'Recognized Phrases',
                Speaker = LookUp(
                    colPhrases,
                    'Offset in Seconds' > Int(audRecordingPlayback.Time) And Outset > Round(
                        audRecordingPlayback.Time,
                        2
                    )
                ).Speaker
            )
        );
        //Then loop through the phrases (colUpdateAllPhrasesForSelectedSpeaker) and update Speaker Lookup to the currently selected speaker (drpSelectSpeaker)
        ForAll(
            colUpdateAllPhrasesForSelectedSpeaker As AllPhrases,
            Patch(
                'Recognized Phrases',
                LookUp(
                    'Recognized Phrases' As CurrentPhrase,
                    CurrentPhrase.'Recognized Phrases' = AllPhrases[@demo_recognizedphrasesid]
                ),
                {'Speaker Lookup': drpSelectSpeaker_Transcript.Selected}
            )
        )
    );
    //Do remaining functions concurrently to save time:
    Concurrent(
    //Reload the Recognized Phrases into a local collection (will playback performance, but will be slow to load for larger Transcripts)
        ClearCollect(
            colPhrases,
            SortByColumns(
                Filter(
                    'Recognized Phrases',
                    Transcript.Transcript = glbSelectedTranscript.Transcript
                ),
                "demo_offsetinseconds",
                SortOrder.Ascending
            )
        ),
    //Refresh Current Phrase (glbCurrentPhrase)
        Set(
            glbCurrentPhrase,
            LookUp(
                ShowColumns(
                    'Recognized Phrases',
                    "demo_display",
                    "demo_durationinseconds",
                    "demo_offsetinseconds",
                    "demo_outset",
                    "demo_phrasenumber",
                    "demo_speaker",
                    "demo_SpeakerLookup",
                    "demo_Transcript",
                    "demo_recognizedphrasesid"
                ),
                demo_offsetinseconds <= Trunc(audRecordingPlayback.Time) And demo_outset >= Trunc(audRecordingPlayback.Time)
            )
        ),
    //Reset variable (glbMode) to View Mode (DisplayMode.View
        Set(
            glbMode,
            DisplayMode.View
        ),
    //Reset Speaker Drop Down
        Reset(drpSelectSpeaker_Transcript)
    );
    //Hide Spinnner
    Set(
        glbShowSpinner,
        false
    );
    ```
  - **Visible**: ```false```

- **shpFileTranscript** </br>Used to properly size the [pdfFileTranscript](#pdfFileTranscript).  
  - **FillPortions**: ```1```  

- **btnDownloadTranscriptFile** </br>When user clicks this button, it uses the Dataverse Web API to get the transcript file attached to the selected transcript
  - **AccessibleLabel**: ```"Click to download transcript PDF"```
  - **Appearance**: ```'ButtonCanvas.Appearance'.Primary```
  - **Icon**: ```"ArrowDownload"```
  - **Layout**: ```"Icon before"```
  - **OnSelect**: For more see: [https://www.matthewdevaney.com/power-apps-download-file-from-dataverse-file-column/](https://www.matthewdevaney.com/power-apps-download-file-from-dataverse-file-column/)
    ```
      Download(
          // Get Web API Endpoint environment variable  and then concat the current transcript GUID and path to the transcript file
          LookUp(
              'Environment Variable Values',
              'Environment Variable Definition'.'Schema Name' = "demo_WebAPIEndpoint"
          ).Value & "/demo_transcripts(" & glbSelectedTranscript.Transcript & ")/demo_transcriptfile/$value"
      );
    ```
  - **Text**: ```"Download"```
  - **Width**: ```120```

- **btnRefreshTranscriptFile** </br>Regenerates the PDF file via Power Automate when clicked
  - **AccessibleLabel**: ```"Click to refresh the transcript PDF"```
  - **Appearance**: ```'ButtonCanvas.Appearance'.Subtle```
  - **Icon**: ```"ArrowClockwise"```
  - **Layout**: ```"Icon before"```
  - **OnSelect**: Shows the Loading Spinner, then runs the flow to regenerate the Transcript PDF
    ```
    //Set loading spinner label
    Set(
        glbSpinnerLabel,
        "Refreshing..."
    );
    //Show loading spinner
    Set(
        glbShowSpinner,
        true
    );
    // Run flow (PA-CreateTranscriptDocument) and store success (Yes or No) in local variable (locTranscriptFileCreated)
    UpdateContext({locTranscriptFileCreated: 'PA-CreateTranscriptDocument'.Run(glbSelectedTranscript.Transcript).success});
    //If successful, notify user of success. 
    If(
        locTranscriptFileCreated = "Yes",
        Notify(
            "Transcript file refreshed",
            NotificationType.Success
        ),
        // If not, notify user of failure
        Notify(
            "Error: Transcript file refreshed",
            NotificationType.Error
        )
    );
    // Refresht the datasource (Transcripts)
    Refresh(Transcripts);
    // Hide loading spinner
    Set(
        glbShowSpinner,
        false
    )
    ```
  - **Text**: ```"Regenerate PDF"```
  - **Width**: ```200```

[▲Top](#contents)

******* 

## Flows
There are six flows in this solution.  Most are designed to run sequentially (hence the numbering).   

- [01 - Power Apps - Upload to Azure Blob](#01---power-apps---upload-to-azure-blob)
- [02 -Child Flow - Create Transcript](#02--child-flow---create-transcript)
  - [02b Child Flow - Loop Until Transcript Complete](#02b-child-flow---loop-until-transcript-complete)
  - [02c Child Flow - Get Transcript Results](#02c-child-flow---get-transcript-results)
  - [02d Child Flow - Parse Transcript and Load into Dataverse](#02d-child-flow---parse-transcript-and-load-into-dataverse)
  - [02e - Child Flow - Summarize Tanscript](#02e---child-flow---summarize-tanscript)
- [PA - Create Transcript Document](#pa---create-transcript-document)

[▲Top](#contents)
### 01 - Power Apps - Upload to Azure Blob
Flow is started when user click Upload button in canvas app.  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/06e337aa-0ad7-4047-961a-3a99162aef71)

_Note there is a parellel branch in this flow. This allows the flow to return a value to the app and effectively end the flow while it continues to execute the child flows in the other branch._ 

Here's a detailed breakdown of each action:
- **Power Apps (V2)*: Triggered by user in canvas app. Passes two parameters from app:
  - **Total Speakers**: Total number of speakers in audio file. Used by Azure Speech to Text services for speaker diarization
  - **File Content**:  Actual audio file. Note: current limitation of 100 Mb
- ** Create blob (V2)**: Uploads file passed from app.  Parameters include:
  - **Storage account name**: Uses Azure Blob credentials (set on import). _Note: You must have an Azure Storage account and container to use this (see [Prerequistes](transcript-demo-power%2Bazure#prerequisites)</i></sub>
  - **Folder Path**: Uses environment variable (set on import) to point to the destination folder
  - **Blob name**: Uses guid() + file name (from trigger) to generate a unique file name every time
  - **Blob content**: Actual content of file
- **Response**: Returns the Body of the Create blob action to the canvas app.
- **Run a Child Flow - Create Transcript**:  See below for more details
    

[▲Top](#contents)
### 02 - Child Flow - Create Transcript  
Master flow that is triggered from the 01 - Power Apps - Upload to Azure Blob flow. Then it transcribes the audio file (via Azure Speech Services) and then calls several more child flows

For more on the Azure Batch Speech to Text transcription click[https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-transcription](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-transcription). 
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/0422b020-a414-4b64-8d25-cce87663307b)

Here's breakdown of each action:
- **Manually trigger a flow**: This child low is triggered by 01 - Power Apps - Upload to Azure Blob flow.  It has three parameters:
  - **TotalSpeakers**: How many speakers should Azure Speech to Text services expect (for diarization)
  - **BlobPath**: Path of the blob uploaded in the 01 - Power Apps - Upload to Azure Blob flow
  - **FileName**: Name of the file uploaded in the 01 - Power Apps - Upload to Azure Blob flow 
- **Intialize variable inMinumSpeakers**:  Set an integer variable to 1 less than the TotalSpeakers value passed to the flow
- **Create SAS URI by path (V2)**: Creates a Shared Access String URI path with read-only permissions set to expire 1 year later.  
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/ccf81c75-1b66-4879-9099-e304773b54ba)

- **HTTP**: Due to limitations at the time of this writing, the solution leverages the [Azure Batch Speech to Text REST API](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/rest-speech-to-text#transcriptions) instead of the Azure Batch Speech to Text connector. I recommend re-factoring to use OOTB connector when possible.
   
 ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/0543949e-40cb-4b80-af3e-b50c5c2718e3)

   Here are the parameters passed:  
  - **Method**: ```POST```
  - **URI**: ```https://XXXXXXX.api.cognitive.microsoft.us/speechtotext/v3.1/transcriptions```
  - **Headers**:
    -   **Content-Type**: ```application/json```
    -   **Ocp-Apim-Subscription-Key**: ```@parameters('Speech To Text Key (demo_SpeechToTextKey)')```
    -   **Body**: 
      ```
      {
        "title": "Transcription",
        "model": null,
        "properties": {
          "diarizationEnabled": true,
          "wordLevelTimestampsEnabled": false,
          "displayFormWordLevelTimestampsEnabled": false,
          "channels": [
            0
          ],
          "destinationContainerUrl": "@{parameters('Azure Blob Destination SAS URL (demo_AzureBlobDestinationSASURL)')}",
          "punctuationMode": "None",
          "profanityFilterMode": "None",
          "diarization": {
            "speakers": {
                        "minCount": "@variables('intMinimumSpeakers')",
                        "maxCount": "@triggerBody()['number']"
            }
          }
        },
        "contentUrls": [
           "@{outputs('Create_SAS_URI_by_path<sub><i>(V2)')?['body/WebUrl']}"
        ],
        "displayName": "transcript-@{utcNow()}"
        "locale": "en-US"
      }
      ```
      
      There are more options you can pass to the REST API. See full documentation [here](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/rest-speech-to-text#transcriptions)
- **Parse JSON**: Parses the body (output) of the HTTP action.
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/08198429-189a-4b8c-b8a2-3b878a4199f9)
   Here are the parameters:
  - **Content**: ```@{body('HTTP')}```
  - **Schema**
     ```
    {
        "type": "object",
        "properties": {
            "self": {
                "type": "string"
            },
            "model": {
                "type": "object",
                "properties": {
                    "self": {
                        "type": "string"
                    }
                }
            },
            "links": {
                "type": "object",
                "properties": {
                    "files": {
                        "type": "string"
                    }
                }
            },
            "properties": {
                "type": "object",
                "properties": {
                    "diarizationEnabled": {
                        "type": "boolean"
                    },
                    "wordLevelTimestampsEnabled": {
                        "type": "boolean"
                    },
                    "displayFormWordLevelTimestampsEnabled": {
                        "type": "boolean"
                    },
                    "email": {
                        "type": "string"
                    },
                    "channels": {
                        "type": "array",
                        "items": {
                            "type": "integer"
                        }
                    },
                    "punctuationMode": {
                        "type": "string"
                    },
                    "profanityFilterMode": {
                        "type": "string"
                    },
                    "diarization": {
                        "type": "object",
                        "properties": {
                            "speakers": {
                                "type": "object",
                                "properties": {
                                    "minCount": {
                                        "type": "integer"
                                    },
                                    "maxCount": {
                                        "type": "integer"
                                    }
                                }
                            }
                        }
                    }
                }
            },
            "lastActionDateTime": {
                "type": "string"
            },
            "status": {
                "type": "string"
            },
            "createdDateTime": {
                "type": "string"
            },
            "locale": {
                "type": "string"
            },
            "displayName": {
                "type": "string"
            }
        }
    }
    ```
  
- **Run a Child Flow - Loop Until Complete**: Calls the child flow () and passes the path (URL) of the transcriptions (from the previous child flow)
- **If Transcript Failed** - Checks if the transcription was successful or not. If it failed, stop the flow and return error.
- **Run a Child Flow - Get Transcript Results**: Calls the child flow () and passes the path of the transcription files (from previous child flow)

From here, there is a parallel branch to allow for two child flows to run concurrently. In one branch:
- **Run a child flow - Summarize Transcript**: Passes the FullTranscript text to the child flow.
  - **FullTranscript**: ```"@first(body('Run_a_Child_Flow_-_Get_Transcript_Results')?['combinedRecognizedPhrases'])?['display']"```
In the other branch, the following actions are called:
- **Get Blob Metadata using path (v2)**: Gets the metadata associated with the blob stored in the storage account. Needed to get file size.  Has two parameters:
  - **Storage account**: Uses Azure Blob credentials (set on import)
  - **Blob path**: ```@triggerBody()['text']```
- **Run a Child Flow - Parse Transcript and Load into Dataverse**: Calls the child flow () and passes the following parameters:  
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/3547c0d4-aa1a-4f60-9f27-fc859b26b7f2)
  - **Transcript**: ```string(outputs('Run_a_Child_Flow_-_Get_Transcript_Results')?['Body'])```
  - **File Name** ```@{triggerBody()['text_1']}```
  - **File Size**: ```@{outputs('Get_Blob_Metadata_using_path<sub><i>(V2)')?['body/Size']}```
The two branches merge back and the flow continues:
-  **Update a row - Transcript Summary**:  Stores the AI Builder generated summary in the Transcript record
-  **Run a child flow - Create Transcript Document**: Passes the transcript Id to the child flow
  - **TranscriptId":  ```@{body('Run_a_Child_Flow_-_Parse_Transcript_and_Load_into_Dataverse')?['demo_transcriptid']}```
- **Response**: Child flows require a repsonse action. No data is passed back to the parent flow
    
[▲Top](#contents)

### 02b Child Flow - Loop Until Transcript Complete
Due to issue/limitation of the [Azure Blob Storage trigger]([url](https://learn.microsoft.com/en-us/connectors/azureblob/)) on file create/update, I had to create a flow that waits for the transcription to complete. Use caution when looping. If possible, re-factor to trigger when transcript file is completed.  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/cf3f6466-3fd5-416b-9699-df0fab9d6e9a)  
Here's a breakdown of each action:  
- **Manually trigger flow**: Child flow is triggered from parent flow [02 - Azure - When Audio File Created in Blob Storage - Create Transcript](#02---azure---when-audio-file-created-in-blob-storage---create-transcript) and receives a text parameter with the transcriptions path
- **Initialize variable  varWait**: Creates a variable with these paramters
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/116b2c78-df66-43a0-939d-8ec2a0f24501)

  - **Name**: ```varWait```
  - **Type**: ```Integer```
  - **Value**: ```500```  
    _Higher the number, the longer the wait_
- **Initialize variable varCompleted**: Creates variable with these parameters:
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/dd18c404-2229-4e65-ab61-cac2eeb6525e)

  - **Name**: ```varCompleted```
  - **Type**: ```Boolean```
  - **Value**: ```@{false}```
- **Do until**:  This loops until **varComplete** is **true**. 
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/1e4d24fa-20e9-4860-aa92-f2d3996b4960)
   Inside the following actions happen for each loop:
  - **Reset variable varWait**: At the start of each loop, reset to 500  - 
  - **HTTP Get Transcript Status**:  Attempts to retrieve the transcription status using the [Azure Batch Speech to Text REST API](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/rest-speech-to-text#transcriptions).  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/89475946-26fd-4464-9f31-fc1d23c480db)  
    With the following parameters:
    - **Method**: ```GET```
    - **URI**: ```@{triggerBody()['text']}```
      _This is the path passed to the flow from the parent flow  [02 - Azure - When Audio File Created in Blob Storage - Create Transcript](#02---azure---when-audio-file-created-in-blob-storage---create-transcript)</i></sub>
    - **Headers**:
      -  **Ocp-Apim-Subscription-Key**: ```@parameters('Speech To Text Key (demo_SpeechToTextKey)')```
  - **If Fail, wait and try again**: A second Do Until loop only runs when the previous action fails.  The HTTP action fails until the transcription has started. This can take several minutes depending on Azure resources.
    The loop ends when varWait equals 0  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/8843295d-a675-4773-8b4f-8666af01cf7d)  
    _Note: the Configure Run After is set to only run this action when the previous action fails_  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/d16a3d5e-d379-4dc5-bce5-10cdc2ada818)  
  - **Parse JSON**: This action (and subsequent actions) only run when the previous action is skipped.  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/a5b034fd-938d-4c72-9683-1642d1a16df6)  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/e08d8dcd-dfeb-4e42-8025-43743c8af9f6)  

    The follow parameters are passed:  
    - **Content**: Pass the output of the **HTTP Get Transcript Status** action
      ```@{body('HTTP_Get_Transcript_Status')}```
    - **Schema**:
      ```
      {
          "type": "object",
          "properties": {
              "self": {
                  "type": "string"
              },
              "model": {
                  "type": "object",
                  "properties": {
                      "self": {
                          "type": "string"
                      }
                  }
              },
              "links": {
                  "type": "object",
                  "properties": {
                      "files": {
                          "type": "string"
                      }
                  }
              },
              "properties": {
                  "type": "object",
                  "properties": {
                      "diarizationEnabled": {
                          "type": "boolean"
                      },
                      "wordLevelTimestampsEnabled": {
                          "type": "boolean"
                      },
                      "displayFormWordLevelTimestampsEnabled": {
                          "type": "boolean"
                      },
                      "channels": {
                          "type": "array",
                          "items": {
                              "type": "integer"
                          }
                      },
                      "punctuationMode": {
                          "type": "string"
                      },
                      "profanityFilterMode": {
                          "type": "string"
                      },
                      "duration": {
                          "type": "string"
                      },
                      "languageIdentification": {
                          "type": "object",
                          "properties": {
                              "candidateLocales": {
                                  "type": "array",
                                  "items": {
                                      "type": "string"
                                  }
                              }
                          }
                      }
                  }
              },
              "lastActionDateTime": {
                  "type": "string"
              },
              "status": {
                  "type": "string"
              },
              "createdDateTime": {
                  "type": "string"
              },
              "locale": {
                  "type": "string"
              },
              "displayName": {
                  "type": "string"
              }
          }
      }
      ```
  - **If Status is Success**: A conditional control that checks if the status is Succeeded  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/e427bb29-fb80-45f5-9270-c60e80538695)  
    - **If Yes**: Then update **varCompleted** to ```true```
      ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/20a888d9-ee4e-4771-a902-43209066957c)
    - If no: Reset the varWait and do another loop (do until):  
      ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/fd7aa5ef-23cd-4540-aa9c-e86e841d227a)  

      - **Reset varWait to 5000**
      - **If not Succeeded, Wait and Try Again**: Do Until **varWait** equals ```0```
        -  **Decrement variable  varWait by 1**


[▲Top](#contents)
### 02c Child Flow - Get Transcript Results
Use [Azure Speech Services REST API](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/rest-speech-to-text#transcriptions) to retrieve the transcription files (report and content) using the Path provided by previous flow. 
NOTE: Due to issues with OOTB Azure Speech Services connector, I leveraged the HTTP connector to call the Azure Speech Services REST API. I recommend re-factoring to use the OOTB connector if/when possible  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/1c282b5c-d74d-47f7-9f9a-3a6a27ab6129)  
Here is a breakdown of each action:
- **Manually trigger a flow**:  Child flow is triggered from parent flow [02 - Azure - When Audio File Created in Blob Storage - Create Transcript](#02---azure---when-audio-file-created-in-blob-storage---create-transcript) and receives a text parameter with the transcription files path
- **HTTP Get Transcript Files**: Gets the files generated by the Azure Batch Speech to Text transcription service (from flow )  
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/26892f48-6ff8-4609-a0a1-8bbfde1e8b95)  
  Here are the parameters passed:
  - **Method**: ```GET```
  - **URI**: ```@{triggerBody()['text']}```
  - **Headers**:
      -  **Ocp-Apim-Subscription-Key**: ```@parameters('Speech To Text Key (demo_SpeechToTextKey)')```
- **Parse JSON**: Takes the output (JSON) from the previous action and parses it
  - **Content**: ```@{body('HTTP_Get_Transcript_Files')}```
  - **Shema**:
    ```
    {
        "type": "object",
        "properties": {
            "values": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "self": {
                            "type": "string"
                        },
                        "name": {
                            "type": "string"
                        },
                        "kind": {
                            "type": "string"
                        },
                        "properties": {
                            "type": "object",
                            "properties": {
                                "size": {
                                    "type": "integer"
                                }
                            }
                        },
                        "createdDateTime": {
                            "type": "string"
                        },
                        "links": {
                            "type": "object",
                            "properties": {
                                "contentUrl": {
                                    "type": "string"
                                }
                            }
                        }
                    },
                    "required": [
                        "self",
                        "name",
                        "kind",
                        "properties",
                        "createdDateTime",
                        "links"
                    ]
                }
            }
        }
    }
    ```
  - **Filter array - contenturl 0.json**: Filters the array of files returned to the one (contenturl_0.json) with the actual transcript  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/10d067f2-7320-41db-b76e-8e6fe75a5fc6)
  - **HTTP Get Transcript**: Get's the actual transcript (JSON) via the REST API  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/b808a290-4a1d-4a67-a1df-777ae1322e5c)  
    Here are the parameters passed:
    - **Method**: ```GET```
    - **URI**: ```@{first(body('Filter_array_-_contenturl_0.json'))?['links']?['contenturl']}```
      **_Note_**: _This flow uses the ```first()``` function to avoid the need for For Each loop. IF you are submitting mutliple files at one time to transcribe, please replace with a For Each control_  
    - **Headers**:
        -  **Ocp-Apim-Subscription-Key**: ```@parameters('Speech To Text Key (demo_SpeechToTextKey)')```
  - **Response**: Sends the full transcript JSON back to the parent flow.  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/1c7045dc-fe01-4cb7-acc5-3b852f32dacb)  

    The paramters are:
    - **Status Code**: ```200```
    - **Body**: ```@{body('HTTP_Get_Transcript')}```
    - **Response Body JSON Schema**:
      ```
      {
          "type": "object",
          "properties": {
              "source": {
                  "type": "string"
              },
              "timestamp": {
                  "type": "string"
              },
              "durationInTicks": {
                  "type": "integer"
              },
              "duration": {
                  "type": "string"
              },
              "combinedRecognizedPhrases": {
                  "type": "array",
                  "items": {
                      "type": "object",
                      "properties": {
                          "channel": {
                              "type": "integer"
                          },
                          "lexical": {
                              "type": "string"
                          },
                          "itn": {
                              "type": "string"
                          },
                          "maskedITN": {
                              "type": "string"
                          },
                          "display": {
                              "type": "string"
                          }
                      },
                      "required": [
                          "channel",
                          "lexical",
                          "itn",
                          "maskedITN",
                          "display"
                      ]
                  }
              },
              "recognizedPhrases": {
                  "type": "array",
                  "items": {
                      "type": "object",
                      "properties": {
                          "recognitionStatus": {
                              "type": "string"
                          },
                          "channel": {
                              "type": "integer"
                          },
                          "speaker": {
                              "type": "integer"
                          },
                          "offset": {
                              "type": "string"
                          },
                          "duration": {
                              "type": "string"
                          },
                          "offsetInTicks": {
                              "type": "integer"
                          },
                          "durationInTicks": {
                              "type": "integer"
                          },
                          "nBest": {
                              "type": "array",
                              "items": {
                                  "type": "object",
                                  "properties": {
                                      "confidence": {
                                          "type": "number"
                                      },
                                      "lexical": {
                                          "type": "string"
                                      },
                                      "itn": {
                                          "type": "string"
                                      },
                                      "maskedITN": {
                                          "type": "string"
                                      },
                                      "display": {
                                          "type": "string"
                                      }
                                  },
                                  "required": [
                                      "confidence",
                                      "lexical",
                                      "itn",
                                      "maskedITN",
                                      "display"
                                  ]
                              }
                          }
                      },
                      "required": [
                          "recognitionStatus",
                          "channel",
                          "speaker",
                          "offset",
                          "duration",
                          "offsetInTicks",
                          "durationInTicks",
                          "nBest"
                      ]
                  }
              }
          }
      }
      ```

[▲Top](#contents)
### 02d Child Flow - Parse Transcript and Load into Dataverse  
Parses the transcript file and loads into Dataverse. One record in the Transcripts table for the transcription and records in the Recognized Phrases for each phrase returned by Azure Speech Services
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/17200620-119d-4a0e-8465-371648e26579)

Here is a breakdown of eacha action:
- **Manually trigger a flow**:  Child flow is triggered from parent flow [02 - Azure - When Audio File Created in Blob Storage - Create Transcript](#02---azure---when-audio-file-created-in-blob-storage---create-transcript) and receives three parameters  
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/e0177581-1daf-417b-99a4-6cd87984af3a)  
  - **Transcript**
  - **File Name**
  - **File Size**
- **Parse JSON**  
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/0c27ca9d-3f9a-4e14-bcc9-2b70ba966e2d)  
  - **Content**: ```@{triggerBody()['text']}```
  - **Schema**:
    ```
    {
        "type": "object",
        "properties": {
            "source": {
                "type": "string"
            },
            "timestamp": {
                "type": "string"
            },
            "durationInTicks": {
                "type": "number"
            },
            "recognizedPhrases": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "recognitionStatus": {
                            "type": "string"
                        },
                        "channel": {
                            "type": "integer"
                        },
                        "speaker": {
                            "type": "integer"
                        },
                        "offsetInTicks": {
                            "type": "number"
                        },
                        "durationInTicks": {
                            "type": "number"
                        },
                        "nBest": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "confidence": {
                                        "type": "number"
                                    },
                                    "display": {
                                        "type": "string"
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
    ```
  - **Add a new row**: Adds a new row to the Transcripts table  
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/416c5a63-ec8b-4c81-bed5-77d70912aa65)  
  With the following parameters: 
    - **Table name**: ```Transcripts```
    - **Duration**: ```@{div(int(body('Parse_JSON')?['durationInTicks']),10000000.00)}```
      - _Note: Uses ```div()``` and ```int()``` functions to return the duration in seconds with two decimal points_
    - **Duration in Ticks**: ```@{int(body('Parse_JSON')?['durationInTicks'])}```
      - _Note: Uses ```int()``` to convert **durationInTicks** from **Parse JSON** into integer_ 
    - **Source File Name**: ```@{triggerBody()['text_1']}```
    - **Source File Size**: ```@{triggerBody()['number']}```
    - **Source URL**: ```@{body('Parse_JSON')?['source']}```
    - **Time Stamp**: ```@{body('Parse_JSON')?['timestamp']}```
- **Apply to each**: For each Recognized Phrase (from Parse JSON), perform the following action:
  - **Add a new row to Recognized Phrases**: Add each recogonized phrase to the Recogonized Phrases table  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/9a6baf65-f178-450d-bc2a-13ef28e0b4b0)  
    with the following parameters:
    - **Table name**: ```Recognized Phrases```
    - **Confidence**: ```@{first(items('Apply_to_each')['nBest'])?['confidence']}```
      - _Note: The ```first()``` function is used to avoid another For Each loop. See [Rest API documentation](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-transcription-get?pivots=rest-api#transcription-result-file) for more on **nBest**_
    - **Display**: ```@{first(items('Apply_to_each')['nBest'])?['display']}```
      - _Note: The ```first()``` function is used to avoid another For Each loop. See [Rest API documentation](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-transcription-get?pivots=rest-api#transcription-result-file) for more on **nBest**_
    - **Duration in Seconds**: ```@{div(int(items('Apply_to_each')?['durationInTicks']),10000000.00)```
      - _Note: Uses ```div()``` and ```int()``` functions to return the duration in seconds with two decimal points_
    - **Duration in Ticks**: ```@{int(items('Apply_to_each')?['durationInTicks'])}```
      - _Note: Uses ```int()``` to convert **durationInTicks** from **Parse JSON** into integer_ 
    - **Offset in Seconds**: ```@{div(int(items('Apply_to_each')?['offsetInTicks']),10000000.00)}```
      - _Note: Uses ```div()``` and ```int()``` functions to return the duration in seconds with two decimal points_
    - **Offset in Ticks**: ```@{int(items('Apply_to_each')?['offsetInTicks'])}```
      - _Note: Uses ```int()``` to convert **offsetInTicks** from **Parse JSON** into integer_ 
    - **Speaker**: ```@{items('Apply_to_each')?['speaker']}```
    - **Transcript (Transcripts)**: ```demo_transcripts(@{outputs('Add_a_new_row')?['body/demo_transcriptid']})```
      - _Note: This relates the recognized phrase to it's parent record in the Transcripts table_
- **Response**: Returns **Status**: ```200``` to parent flow if no issues/errors.
  
[▲Top](#contents)

### 02e - Child Flow - Summarize Tanscript
Uses AI Builder GPT action to generate a summary of the transcript

![02e - Child Flow - Summarize Tanscript](https://github.com/user-attachments/assets/c4bf2688-7547-46ec-a1ec-e574b6049ab8)

Here is a breakdown of each action
- **Manual trigger a flow**: Child flow is triggered from parent flow [02 - Azure - When Audio File Created in Blob Storage - Create Transcript](#02---azure---when-audio-file-created-in-blob-storage---create-transcript) and receives 1 parameter:
        - **FullTranscript**: The full text of the transcript
- **Create text with GPT using a prompt** :  Uses the OOTB **AI Summarize** prompt to generate a summary of the full transcript
- **Start and wait for an approval**:  As part of our responsible AI policies, we require a human to review all generative AI responses from AI Builder... HOWEVER, in this case, I've set this as a Static Result action, so it just continues to the next action without requesting approval.  Without this action, the flow would fail.
  
  ![Screenshot of Static Result](https://github.com/user-attachments/assets/5c8a4773-1ba2-4743-9b9e-90fad95c287d)
  
- **Response** - This returns the text generated by AI Builder

[▲Top](#contents)

### PA - Create Transcript Document
Generates a Word and PDF doc version of the transcript based on Transcript GUID passed to flow. 

![PA - Create Transcript Document](https://github.com/user-attachments/assets/6826c806-8469-45d3-970c-b4819bf53dd5)

This flow is triggered by parent flow [02 - Azure - When Audio File Created in Blob Storage - Create Transcript](#02---azure---when-audio-file-created-in-blob-storage---create-transcript) OR by the user using the canvas app.  

Here is a breakdown of each action
- **Power Apps (V2)**: Triggered either via flow or canvas app. Passes one parameter:
  - **Transcriptid** : GUID of the Transcript record
- **Get Transcript**: Retrieve the Transcript record that matches the Transcriptid passed to the flow
- **List Recognized Phrases**: Retrieves all Recognized Phrases related to the Transcript record and sorts them in ascending order by offset in seconds
  
  ![image](https://github.com/user-attachments/assets/79288d60-d4f7-42cf-9975-7c88b9a7ae5d)
  
  - **Table Name**: ```Recognized Phrases```
  - **Filter Rows**: ```_demo_transcript_value eq '@{triggerBody()['text']}'```
  - **Sort By**: ```demo_offsetinseconds asc```
- **Select Phrases**: Builds an array object with only three fields
  
  ![image](https://github.com/user-attachments/assets/fdc05397-13a0-4669-a913-bf64d08f0b31)
  
  - **From**: ```@{outputs('List_Recognized_Phrases')?['body/value']}```
  - **TimeStamp**: ```@item()?['demo_timestamp']```
  - **Speaker**: ```@coalesce(item()?['_demo_speakerlookup_value@OData.Community.Display.V1.FormattedValue'],item()?['demo_speaker'])```
  - **Phrase**: ```@item()?['demo_display']```
- **Populate a Microsoft Word template**: Populates a Word template with transcript
  
  ![image](https://github.com/user-attachments/assets/0f19f8d8-1df6-4791-b4f6-e1c531afec20)

  - **Location**: Environment variable **demoSharePointSite**
    ```
    @parameters('SharePoint Site (demo_SharePointSite)')
    ```
  - **Document Library**:   It must be stored in a SharePoint document library
    ```
    Documents
    ```
    - Note: due to limitations of this connector, you must hardcode the word template location and file name.
  - **File**: ```/General/Transcript Demo Template.docx```
    - Please update with your template
  - **TranscriptNumberHeader**: ```@{outputs('Get_Transcript')?['body/demo_transcriptnumber']}```
  - **PhraseRows**: Output of the previous action (Select Phrases)
    ```@body('Select_Phrases')```
  - **CreatedBy**: ```@{replace(outputs('Get_Transcript')?['body/_createdby_value@OData.Community.Display.V1.FormattedValue'],'# ','')}```
    - _The replace() function is there to remove a '#' suffix that was in my tenant. You may or may not have that issue_  
  - **FileNameHeader**: ```@{outputs('Get_Transcript')?['body/demo_sourcefilename']}```
  - **Summary**: ```@{outputs('Get_Transcript')?['body/demo_summary']}```
  - **TranscriptNumber**: ```@{outputs('Get_Transcript')?['body/demo_transcriptnumber']}```
  - **FileName**: ```@{outputs('Get_Transcript')?['body/demo_sourcefilename']}```
  - **Duration**: ```@{outputs('Get_Transcript')?['body/demo_durationhhmmss']}```
  - **CreateDate**: ```formatDateTime(outputs('Get_Transcript')?['body/createdon'],'MM/dd/yyyy')```

- **Create Word Doc (Temporary)**: Creates a (temporary) Word document file
  
  ![image](https://github.com/user-attachments/assets/b6183476-3fc7-45f1-b369-38cdddff41b8)

  - **Folder Path**: ```/```
    - This demo uses your OneDrive's root folder. Feel free to update to another preferred location
  - **File Name**: Concantenates the source file name with the Word file extention (.docx)
    ```
    @{outputs('Get_Transcript')?['body/demo_sourcefilename']}.docx
    ```
  - **File Content**: ```@{body('Populate_a_Microsoft_Word_template')}```
    
- **Convert file**: Converts the Word doc to PDF
  
  ![image](https://github.com/user-attachments/assets/0f6b044c-4cf3-4350-abbd-de4dea32937b)

  - **File**: ```@outputs('Create_Word_Doc_(Temporary)')?['body/Id']```
  - **Target type**: ```PDF```
- **Upload file or image**: Attaches the PDF to the Transcript record
  
  ![image](https://github.com/user-attachments/assets/5c9f1851-86fa-4632-8f80-2cf2a0e04439)

  - **Table name**: ```Transcripts```
  - **Row ID**: ```@{triggerBody()['text']}```
  - **Column name**: ```Transcript File```
  - **Content**: ```@{body('Convert_file')}```
  - **Content name**: ```@{outputs('Convert_file')?['headers/x-ms-file-name']}```

- **Delete (temporary) Word Doc**: Removes the temporary Word doc to save space
  
  ![image](https://github.com/user-attachments/assets/bc0df719-62bc-446e-a227-a20d790d7e6f)

  - **File**: ```@outputs('Create_Word_Doc_(Temporary)')?['body/Id']```
- **Respond to a Power App or flow**: A response is required for Child flows. This simply returns **Yes** if the flow is successful
  
  ![image](https://github.com/user-attachments/assets/7dd7db37-1692-40c8-b751-ef9ca6993902)

- **Respond to Power App or flow - Error Handler**: If the flow fails, returns **No**
  
  ![image](https://github.com/user-attachments/assets/c5d24809-1df6-4f15-8b91-a6fbb159810d)

  - The **Configure run after** setting is set to only run this action if the previous action is skipped  (i.e. there was a failure)
    
    ![image](https://github.com/user-attachments/assets/09436986-f682-4bc9-abaa-68c2a098b7a4)


[▲Top](#contents)
